---
title: "Lab 6: One-Way Within-Subjects ANOVA"
output:
  html_document:
    toc: true
    toc_depth: 3
---
## 1. Introduction

This tutorial will learn how to run one-way within-subjects ANOVA. While in many ways the analysis will resemble one-way between subjects, the primary difference will be in the structure of the data. 

We will use data from your textbook (see ['Support Materials' on the textbook webpage](https://www.routledge.com/Design-and-Analysis-in-Educational-Research-ANOVA-Designs-in-SPSS/Strunk-Mwavita/p/book/9781138361164)) (Felver, Morton, & Clawson 2018). For your Lab 6 assignment, you will use the data provided on Canvas.

### 1.2 Background
In this article, the researchers report on their work around mindfulness training for college students. Specifically, they were interested in testing whether psychological outcomes would improve after a mindfulness-based stress reduction program. They followed students across a pre-test, a post-test after eight weeks of the program, and a follow-up eight weeks after the post-test. The follow-up test is important to this design because it allowed the researchers to assess whether any differences found at post-test were maintained after the program ended.

**Research Questions**

The authors asked several research questions; in this tutorial, we will focus on one: 

*Was psychological distress, as measured by the global severity index, significantly different at post-test and the follow-up than it was before the mindfulness program?*

The authors measured the global severity index as an indicator of psychological distress. This score came from the Brief Symptom Inventory, which is an 18-item scale that yields several subscale scores, including the global severity index. 


## 2. Data Preparation

Doing within-subjects ANOVA requires restructuring the data in a format that's better to work with.

```{r setup, message = FALSE}
needs(tidyverse, rio, here, knitr, kableExtra, misty, janitor, rstatix, psych, misty, MOTE)
df <- misty::read.sav(here("kamden-mwavita_data/within-subjects ANOVA - Felver, Morton, and Clawson.sav"), use.value.labels = TRUE) %>% 
  clean_names()
```

### 2.1 Preparing Variables and Labels
First, let's view the data by looking at the top five rows.
```{r}
head(df)
```


Notice that this dataset doesn't have an ID column. We need to add that in, and the `rowid_to_column()` function is an easy way to do that. We can call the column `student_id`. If your dataset already has an ID column, you don't need to do this. 

Also, for later use, let's rename the columns with student test scores to make them easier to interpret.
```{r}
df <- df %>% 
  rowid_to_column("student_id") %>% # Creating our ID column
  rename(pre_test = pre_gsi, # Renaming the columns w/ student scores
         post_test = post_gsi,
         followup = followup_gsi)

head(df) # Check to see make sure changes occurred
```

### 2.2 Pivoting the Data from 'Wide' to 'Long'
Our data are currently organized in a *wide* format, with multiple scores for each student stored over multiple columns. To run an our analyses, the data need to be in a *long* format, where all the scores are in one column, so we will create a new column (variable) to store the type of scores (pre, post, or followup). 

We do this with a function called `pivot_longer`. This function takes a few arguments:

* `cols = c()` : inside the `c()`, you specify what columns you want to turn into rows. In this case, that means `pre_test`, `post_test`, and `followup`. You put them inside the `c` and parentheses so it is grouped together, separate from the rest of the function. 
* `names_to = `: this is where you specify what the column names are. Here, all the columns are the type of test, so you can put "test_type".
* `values_to = `: this is where you specify where the numbers actually go. In this case, the numbers all represent scores, so you can put "test_score".

This is what it looks like all together:
```{r}
df <- df %>% 
  pivot_longer(cols = c(pre_test, post_test, followup), 
                        names_to = "test_type",
                        values_to = "test_score")

# Check if it worked
head(df)
```

Now we have one column for student id, one for test type, and one for test score. You can see each student is listed three times, once for each score.

### 2.3 Checking Variable Types

Now, we need to check that each variable is the right data type.
```{r}
str(df)
```

The code above shows us that there are two problems: `student_id` is being counted as "int" (or integer) and `test_type` is being counted as "chr" (or character, which is basically like a string of letters). Both of these need to be factors (`test_score` is fine as a continuous dependent variable). Additionally, for `test_type`, R doesn't know what order to put the tests, so we need to specify the order of levels using the code below (similar to what we went over in Lab 2/3).

We'll use the code below to get the other two in the right format.
```{r}
df$student_id <- as.factor(df$student_id) # convert to a factor

df$test_type <- factor(df$test_type, levels =  c("pre_test", "post_test", "followup")) # convert to a factor and make sure levels are in the right order

#Check if it worked:
str(df)
```

## 3. Data Exploration and Assumptions

We can see the overall distribution of test scores.
```{r}
describe(df)
```

We can also look at more helpful statistics with `describeBy`, examining scores by test_type and/or by student.
```{r}
describeBy(df, group = "test_type")
```

Using the boxplots and descriptions, you have idea now of if the normality assumptions are met. **Note. You don't need to run leveneTest() for within-subjects**

### 3.1 Boxplot
Let's use boxplots to look at the score distributions.
```{r}
boxplots <- df %>% 
  ggplot(aes(x = test_type, y = test_score)) +
  geom_boxplot() +
  theme_minimal() #I like using this theme because it gets rid of the gray background

boxplots
```

At this point, you should have enough information to check all assumptions except for the _assumption of sphericity_. For that, we need to run Mauchly's test which is automatically produced using the `afex` package which will be included in section *4. Conducting One-Way Within-Subjects ANOVA*.

### 3.2 Line Graph
*You don't need to run for your analysis in this lab (6)*, but it might be useful for visualizing the variability that is tested within-subjects analysis. In this case, we've testing the variation of test scores within students over time.

First, set up the base of the graph using `ggplot` and `geom_point()`, which will plot the score of each test, then add `geom_line()`, which will connect the points so it will be easy to track visually.
```{r}
df %>% 
  ggplot(aes(test_type, test_score, group = student_id)) +
  geom_line(aes(color = student_id)) +
  theme_minimal()
```

*Note.* With so many students, it is hard to match `student_id` with their test trajectories. There is a neat tool from the `ggrepel` package that allows you to label each line. That has a lot of new code unrelated to this lab, so just reach out if you are interested and and we'll connect you with resources to help you display data in cool ways.

## 4. Conducting One-Way Within-Subjects ANOVA

We use `aov_car` to allow us to run `emmeans`. This function is a part of the `afex` package. This function takes more information than what you might have seen so far with between-subjects tests. For this, we need to specify our ID variable and the independent variables within the `Error()` term.

The general structure for one-way within-subjects is as follows:

DV ~ IV + Error(ID/IV),

where you plug in your IV, DV, and ID variable. Here is the example:
```{r}
needs(afex)
m1 <- aov_car(test_score ~ test_type + Error(student_id/test_type),
              data = df, include_aov = F)

summary(m1)
```

First, we see that Mauchly's Test for Sphericity produced a test statistic was significant ($W_2$ = .251, _p_ < .001), indicating that the assumption sphericity was not met. We will then interpret the results under the heading "Greenhouse-Geisser and Huynh-Feldt Corrections for Departure from Sphericity", and report the Greenhouse-Geisser correction, where we find a significant difference in the global severity score among the three timepoints (*F* = 8.29, *p* = .010)

We can also find the effect size $\eta_p^{2}$ using the code below.
```{r}
effectsize::eta_squared(m1)
```

Approximately 39% of the variance in global severity scores was explained by change between the pre-test, post-test, and follow up test ($\eta_p^{2}$ = 0.39).

## 5. Running Post-Hoc Custom Contrasts

For the contrasts, we'll use the research question at the beginning of the lab to inform our research question: "Was psychological distress, as measured by the global severity index, significantly different at post-test and the follow-up than it was before the mindfulness program?"

Remember the levels of your test variable before you set the custom contrast:
```{r}
levels(df$test_type)
```

To set up the contrast, we will compare `pre_test` scores against the other two (`post_-_test` and `followup`) using the following weights: (1, -.5, -.5)

First, gather pairwise comparisons for each test.
```{r}
needs(emmeans)

em <- emmeans(m1, ~test_type)

pairs(em)
```

This allows us to see the order of factors to specify coding.
```{r}
em
```

Set the custom contrast:
```{r}
contrasts <- list(
                  hyp1 = c(1, -.5, -.5) # if you have more hypotheses, you would place them here, too
                  )
contrast(em, contrasts)
```

Using the t-ratio of 2.587 and df of 13 from above, we can find the effect sizes calculated below. With corrections for two tests, our new alpha is .05/2 = .025

### 5.1 Effect Sizes

Calculate effect size with partial eta squared ($\eta_p^{2}$).
```{r}
effectsize::t_to_eta2(
                      t = 2.587,
                      df_error = 13
                      )
```

Calculate effect size with omega squared ($\omega^{2}$).
```{r}
#Let's calculate effect size with Omega
# First turn t-ratio into F-ratio by squaring it: t^2 = F
(2.587)^2
# Fvalue = 6.692569

# Second, put in values accordingly:
omega.F(dfm = 1, dfe = 13,
      Fvalue = 6.692569, n = 36, a = .025)
```

While the pre-test differs from the other tests by 5.15 points, the difference is not significant (p = .022). $\eta_p^{2}$ = 0.34 indicated that 34% of the variance between the pre-test score and scores from the other two tests combined, while $\omega^{2}$ = 0.13, indicating that only 13% of the variance between the pre-test score and scores from the other two tests.